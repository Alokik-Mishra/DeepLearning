{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECBM E4040 Assignment 1, Task 4: Questions\n",
    "1) Cross entropy is a metric that measures the \"distance\" between two distributions, why can it be used in calculating the loss of softmax classifier? \n",
    "\n",
    "   Your answer: Cross entropy can look at the actual distirbution of the labeled data into the k classes vs the distribution predicted by the model. The further apart the distributions, the more occurances of missclassification by our model. Thus minimizing the cross-entropy helps in reducing the number of incorrectly classfied observations.\n",
    "\n",
    "\n",
    "2) How does binary SVM classifier deal with multi-class classification problem? In general, there are two ways.Please describe both training process and inference process of different methods.\n",
    "\n",
    "   Your answer: The two ways to deal with multiclass problems are (i) train k one v all classifiers and (ii) train (k * ((k-1)/2)) classfiers.\n",
    "   \n",
    "   (i) For the one v all, we train each classfier by combining all the other k-1 classes into a single class and then determine the weights. In the inference stage we classify based on whichever classfier had the highest likelihood.\n",
    "   (ii) For the all v all, we train a large number of smaller classifiers and compare the probability of being in each class to all other classes seperately. In inference we classify based on the largest number of classifiers with the same predicted label.\n",
    "   \n",
    "\n",
    "3) What are the pros and cons of SVM compared with MLP?\n",
    "\n",
    "   Your answer: Some of the pros is that SVMs find global optimums and generally have fewer hyperparametes. Some of the cons are that for MLP you dont have to decide on the features and the model itself will do the feature engineering. Also (unless you are using kernalised SVMs) MLPs are better at handling nonlinearities in the data.\n",
    "   \n",
    "   \n",
    "\n",
    "4) Why is the ReLU activation function used the most often in neural networks for computer vision?\n",
    "\n",
    "   Your answer: This is because they resemble properties of biological neurons, where for a large range the neuron remains inactive, but past a certiain thershold the neruon is active and its output is proportional to the input. There is also the benefit that during backprop there is no saturation (compared to sigmoid or tanh).\n",
    "   \n",
    "   \n",
    "5) Describe your best model in the implementation of the two-layer neural network. Describe your starting point, how you tuned  hyperparameters, which stategies you used to improve the network, show the results of intermediate and the final steps.\n",
    "\n",
    "   Your answer: In my best model, I kept SGD without momentum, but changed the hidden_dimensions to 200 and the batch_size to 100. My first thought was to increase the number of hidden_dimensions and while I noticed a benefit in increasing to 200, increasing to 300 was not much better. Then I considered two things, changing the learning rate or changing the batch size to get closer to online learning. Look at the previous ouputs, I saw that while at times the loss increased (the learning rate was too big), this was fairly rare. Thus I decided to go with the decearing the batch size, and changing it to 100 i got a accuracy of greater than 0.5.\n",
    "   \n",
    "\n",
    "6) **Cross validation** is a technique used to prove the generalization ability of a model and can help you find a robust set of hyperparameters. Please describe the implementation details of **k-fold cross validation**.\n",
    "\n",
    "   Your answer: In a k-fold cross validation, you randomly split the data into k different disjoint sets. Then for for each set, you leave it out while training the other k-1 sets, and use the left out set to validate the model. Thus each of the k sets is used 1 time for validation and k-1 times for training. The accuracy of the model may be judged by averaging the accuracy over all the sets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
